{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG) Chatbot over Course Materials\n",
        "\n",
        "## Project Overview\n",
        "This project implements a **Retrieval-Augmented Generation (RAG)** application that allows users to ask questions about university course materials (.pdf)\n",
        "\n",
        "The system retrieves relevant document chunks using semantic search and generates answers using an open-source Hugging Face language model.\n",
        "If the answer is not present in the documents, the system responds **\"I don't know\"** to prevent hallucinations.\n"
      ],
      "metadata": {
        "id": "mEmfn0Uiuwod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  System Architecture\n",
        "\n",
        "The RAG system follows a standard pipeline:\n",
        "\n",
        "1. PDF and TXT Documents loading\n",
        "2. Text chunking\n",
        "3. Embedding with Sentence-Transformers\n",
        "4. Vector storage using Chroma\n",
        "5. Retrieval of relevant chunks\n",
        "6. Answer generation using a Hugging Face LLM\n",
        "\n",
        "This architecture separates **retrieval** from **generation**, improving factual accuracy.\n"
      ],
      "metadata": {
        "id": "c045jUiju-cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  langchain==0.1.16 \\\n",
        "  langchain-community==0.0.36 \\\n",
        "  langchain-core==0.1.48 \\\n",
        "  langchain-text-splitters==0.0.1 \\\n",
        "  chromadb sentence-transformers transformers pypdf accelerate gradio\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QHo2UnYskuhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "print(\"RetrievalQA imported successfully âœ…\")\n"
      ],
      "metadata": {
        "id": "qWcVovTNlTKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Document Collection\n",
        "\n",
        "Course materials are stored in Google Drive and loaded automatically.\n"
      ],
      "metadata": {
        "id": "0_NnsT03vRDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/Course_Materials_RAG\"\n",
        "CHROMA_DIR = \"/content/chroma_db\"\n",
        "\n",
        "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "zlr9x50EnIJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents Loading\n",
        "def load_documents(folder):\n",
        "    docs = []\n",
        "    for file in Path(folder).glob(\"**/*\"):\n",
        "        if file.suffix.lower() == \".pdf\":\n",
        "            docs.extend(PyPDFLoader(str(file)).load())\n",
        "        elif file.suffix.lower() in [\".txt\", \".md\"]:\n",
        "            docs.extend(TextLoader(str(file), encoding=\"utf-8\").load())\n",
        "    return docs\n",
        "\n",
        "documents = load_documents(DATA_DIR)\n",
        "print(f\"Loaded {len(documents)} documents\")\n"
      ],
      "metadata": {
        "id": "Ib1J4dQOnKG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Text Chunking\n",
        "\n",
        "Documents are split into overlapping chunks to preserve semantic continuity.\n"
      ],
      "metadata": {
        "id": "S2c_wA_XvnSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=150\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Created {len(splits)} chunks\")"
      ],
      "metadata": {
        "id": "RW7N7LdcnVrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Embeddings and Vector Database\n",
        "\n",
        "Chunks are embedded using a Sentence-Transformer model and stored in Chroma."
      ],
      "metadata": {
        "id": "eTeaZebOvtwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "z5O1J6wxnXM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=CHROMA_DIR\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "uJvBQce4neab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Language Model\n",
        "\n",
        "An open-source instruction-tuned Hugging Face model is used for generation.\n"
      ],
      "metadata": {
        "id": "yvw6AOLXv0GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_NAME = \"google/flan-t5-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_NAME)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "E8PtlatFpaBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Hallucination Control !\n",
        "\n",
        "The model is forced to answer **only from retrieved context**.\n"
      ],
      "metadata": {
        "id": "aOzDiJKXv45A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "RAG_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "You are an academic assistant.\n",
        "Answer the QUESTION using ONLY the CONTEXT below.\n",
        "\n",
        "Rules:\n",
        "-Answer in the same language as the context.\n",
        "- Do NOT use external knowledge.\n",
        "- Do NOT invent information.\n",
        "- If the answer is NOT explicitly contained in the CONTEXT, reply exactly:\n",
        "  \"I don't know\"\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "-qMIJc_2qmD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Chain (CORE)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": RAG_PROMPT},\n",
        "    return_source_documents=False\n",
        ")"
      ],
      "metadata": {
        "id": "NsQfLXCDpwIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  System Evaluation\n"
      ],
      "metadata": {
        "id": "egKVFm4NwHys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_rag(question):\n",
        "    return qa_chain.run(question)"
      ],
      "metadata": {
        "id": "hFxHD6vGpxrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_rag(\"C'est quoi un agent intelligent ?\"))"
      ],
      "metadata": {
        "id": "bw7Zy1NMp0Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_rag(user_message, history):\n",
        "    \"\"\"\n",
        "    Simple RAG chat:\n",
        "    - Question\n",
        "    - Retrieval\n",
        "    - Strict answer from documents\n",
        "    \"\"\"\n",
        "    try:\n",
        "        answer = qa_chain.run(user_message)\n",
        "    except Exception as e:\n",
        "        answer = f\"Error: {str(e)}\"\n",
        "\n",
        "    history = history + [(user_message, answer)]\n",
        "    return history, history, \"\"\n",
        "\n",
        "with gr.Blocks(title=\"ðŸ“˜ Course RAG Assistant\") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ðŸ“˜ Course RAG Assistant\n",
        "    \"\"\")\n",
        "\n",
        "    chatbot = gr.Chatbot(height=400)\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(\n",
        "            placeholder=\"Ask a question about the course...\",\n",
        "            show_label=False\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        send = gr.Button(\"Send\")\n",
        "        clear = gr.Button(\"Clear\")\n",
        "\n",
        "    send.click(\n",
        "        chat_rag,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[chatbot, chatbot, msg]\n",
        "    )\n",
        "\n",
        "    msg.submit(\n",
        "        chat_rag,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[chatbot, chatbot, msg]\n",
        "    )\n",
        "\n",
        "    clear.click(lambda: ([], \"\"), outputs=[chatbot, msg])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "d1KuQxygp9dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Conclusion\n",
        "\n",
        "This project demonstrates a complete vanilla RAG pipeline using open-source tools.\n",
        "By combining semantic retrieval with controlled generation, the system provides accurate and reliable answers grounded in course materials.\n",
        "\n",
        "**Limitations:**  \n",
        "The system depends on the quality and coverage of the provided documents; questions outside this scope are intentionally rejected.\n",
        "\n"
      ],
      "metadata": {
        "id": "IqZucPE2wMON"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7zV77d_xI8J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
